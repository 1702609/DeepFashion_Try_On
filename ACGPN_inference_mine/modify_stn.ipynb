{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314c6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import os.path as osp\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "from PIL import ImageDraw\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "SIZE = 320\n",
    "NC = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b5b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    name = 'label2city'\n",
    "    gpu_ids = [0, 1, 2, 3]\n",
    "    checkpoints_dir = './checkpoints'\n",
    "    model = 'pix2pixHD'\n",
    "    norm = 'instance'\n",
    "    use_dropout = True\n",
    "    verbose = True\n",
    "    batchSize = 1\n",
    "    loadSize = 512\n",
    "    fineSize = 512\n",
    "    label_nc = 20\n",
    "    input_nc = 3\n",
    "    output_nc = 3\n",
    "    dataroot = '../../after_vton_difficult_v2/'\n",
    "    datapairs = 'test_pairs.txt'\n",
    "    resize_or_crop = 'scale_width'\n",
    "    serial_batches = True\n",
    "    no_flip = True\n",
    "    nThreads = 2\n",
    "    max_dataset_size = float(\"inf\")\n",
    "    display_winsize = 512\n",
    "    tf_log = True\n",
    "    netG = 'global'\n",
    "    ngf = 64\n",
    "    n_downsample_global = 4\n",
    "    n_blocks_global = 4\n",
    "    n_blocks_local = 3\n",
    "    n_local_enhancers = 1\n",
    "    niter_fix_global = 0\n",
    "    continue_train = True\n",
    "    display_freq = 100\n",
    "    print_freq = 100\n",
    "    save_latest_freq = 1000\n",
    "    save_epoch_freq = 10 \n",
    "    no_html = True\n",
    "    debug = True\n",
    "    load_pretrain = '../label2city'\n",
    "    which_epoch = 'latest'\n",
    "    phase = 'test'\n",
    "    niter = 100\n",
    "    niter_decay = 100\n",
    "    beta1 = 0.5\n",
    "    lr = 0.0002\n",
    "    num_D = 2\n",
    "    n_layers_D = 3\n",
    "    ndf = 64\n",
    "    lambda_feat = 10.0\n",
    "    no_ganFeat_loss = True\n",
    "    no_vgg_loss = False\n",
    "    no_lsgan = True\n",
    "    pool_size = 0\n",
    "    isTrain = True\n",
    "\n",
    "opt = Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bba6e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dir):\n",
    "    images = []\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "    f = dir.split('/')[-1].split('_')[-1]\n",
    "    dirs= os.listdir(dir)\n",
    "    for img in dirs:\n",
    "        path = os.path.join(dir, img)\n",
    "        #print(path)\n",
    "        images.append(path)\n",
    "    return images\n",
    "\n",
    "def get_params(opt, size):\n",
    "    w, h = size\n",
    "    new_h = h\n",
    "    new_w = w\n",
    "    if opt.resize_or_crop == 'resize_and_crop':\n",
    "        new_h = new_w = opt.loadSize            \n",
    "    elif opt.resize_or_crop == 'scale_width_and_crop':\n",
    "        new_w = opt.loadSize\n",
    "        new_h = opt.loadSize * h // w\n",
    "\n",
    "    x = random.randint(0, np.maximum(0, new_w - opt.fineSize))\n",
    "    y = random.randint(0, np.maximum(0, new_h - opt.fineSize))\n",
    "\n",
    "def get_transform(method=Image.BICUBIC, normalize=True):\n",
    "        transform_list = []\n",
    "        transform_list += [transforms.ToTensor()]\n",
    "        if normalize:\n",
    "            transform_list += [transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                    (0.5, 0.5, 0.5))]\n",
    "        return transforms.Compose(transform_list)\n",
    "    \n",
    "class BaseDataset(data.Dataset):\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        super(BaseDataset, self).__init__()\n",
    "        \n",
    "        human_names = []\n",
    "        cloth_names = []\n",
    "        with open(os.path.join(opt.dataroot, opt.datapairs), 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                h_name, c_name = line.strip().split()\n",
    "                human_names.append(h_name)\n",
    "                cloth_names.append(c_name)\n",
    "        self.human_names = human_names\n",
    "        self.cloth_names = cloth_names\n",
    "        \n",
    "    def image_for_pose(self, pose_name, transform):\n",
    "        with open(osp.join(pose_name), 'r') as f:\n",
    "            pose_label = json.load(f)\n",
    "            pose_data = pose_label['people'][0]['pose_keypoints']\n",
    "            pose_data = np.array(pose_data)\n",
    "            pose_data = pose_data.reshape((-1,3))\n",
    "        point_num = pose_data.shape[0]\n",
    "        fine_height = 256\n",
    "        fine_width = 192\n",
    "        pose_map = torch.zeros(point_num, fine_height, fine_width)\n",
    "        r = 5\n",
    "        im_pose = Image.new('L', (fine_width, fine_height))\n",
    "        pose_draw = ImageDraw.Draw(im_pose)\n",
    "        for i in range(point_num):\n",
    "            one_map = Image.new('L', (fine_width, fine_height))\n",
    "            draw = ImageDraw.Draw(one_map)\n",
    "            pointx = pose_data[i,0]\n",
    "            pointy = pose_data[i,1]\n",
    "            if pointx > 1 and pointy > 1:\n",
    "                draw.rectangle((pointx-r, pointy-r, pointx+r, pointy+r), 'white', 'white')\n",
    "                pose_draw.rectangle((pointx-r, pointy-r, pointx+r, pointy+r), 'white', 'white')\n",
    "            one_map = transform(one_map.convert('RGB'))\n",
    "            pose_map[i] = one_map[0]\n",
    "            return pose_map\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        c_name = self.cloth_names[index]\n",
    "        h_name = self.human_names[index]\n",
    "        A_path = osp.join(self.opt.dataroot, self.opt.phase ,'test' + '_label', h_name.replace(\".jpg\", \".png\"))\n",
    "        label = Image.open(A_path).convert('L')\n",
    "\n",
    "        B_path = osp.join(self.opt.dataroot, self.opt.phase ,'test' + '_img', h_name)\n",
    "        image = Image.open(B_path).convert('RGB') \n",
    "        mask = Image.open(B_path).convert('L')\n",
    "        \n",
    "        E_path = osp.join(self.opt.dataroot, self.opt.phase ,'test' + '_edge', c_name)\n",
    "        edge = Image.open(E_path).convert('L')\n",
    "                \n",
    "        C_path = osp.join(self.opt.dataroot, self.opt.phase ,'test' + '_color', c_name)\n",
    "        color = Image.open(C_path).convert('RGB')\n",
    "        \n",
    "        transform_A = get_transform(method=Image.NEAREST, normalize=False)\n",
    "        label_tensor = transform_A(label) * 255\n",
    "        transform_B = get_transform()      \n",
    "        image_tensor = transform_B(image)\n",
    "        mask_tensor = transform_A(image)\n",
    "        edge_tensor = transform_A(edge)\n",
    "        color_tensor = transform_B(color)\n",
    "        pose_name = osp.join(self.opt.dataroot, self.opt.phase ,'test' + '_pose', h_name.replace('.jpg', '_keypoints.json'))\n",
    "        pose_map = self.image_for_pose(pose_name, transform_B)\n",
    "        \n",
    "        #sigmoid = nn.Sigmoid()\n",
    "        arm_label = torch.load(osp.join(self.opt.dataroot, self.opt.phase, 'msw', c_name.replace('.jpg','.pt')), map_location=torch.device('cuda'))\n",
    "        #arm_label = sigmoid(arm_label)\n",
    "        \n",
    "        fake_cl = torch.load(osp.join(self.opt.dataroot, self.opt.phase, 'msc', c_name.replace('.jpg','.pt')), map_location=torch.device('cuda'))\n",
    "        \n",
    "        return {'label': label_tensor, 'image': image_tensor, \n",
    "                             'edge': edge_tensor,'color': color_tensor, \n",
    "                             'mask': mask_tensor, 'name' : c_name,\n",
    "                             'colormask': mask_tensor,'pose':pose_map,\n",
    "                             'msw': arm_label, 'msc':fake_cl}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.human_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a93c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = BaseDataset(opt)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    t,\n",
    "    batch_size=1,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b1fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def changearm(old_label):\n",
    "    label=old_label\n",
    "    arm1=torch.FloatTensor((label.cpu().numpy()==11).astype(np.int))\n",
    "    arm2=torch.FloatTensor((label.cpu().numpy()==13).astype(np.int))\n",
    "    noise=torch.FloatTensor((label.cpu().numpy()==7).astype(np.int))\n",
    "    label=label*(1-arm1)+arm1*4\n",
    "    label=label*(1-arm2)+arm2*4\n",
    "    label=label*(1-noise)+noise*4\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e8fd133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def gen_noise(shape):\n",
    "    noise = np.zeros(shape, dtype=np.uint8)\n",
    "    ### noise\n",
    "    noise = cv2.randn(noise, 0, 255)\n",
    "    noise = np.asarray(noise / 255, dtype=np.uint8)\n",
    "    noise = torch.tensor(noise, dtype=torch.float32)\n",
    "    return noise.cuda()\n",
    "\n",
    "def encode_input(label_map, clothes_mask,all_clothes_label):\n",
    "    size = label_map.size()\n",
    "    oneHot_size = (size[0], 14, size[2], size[3])\n",
    "    input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n",
    "    input_label = input_label.scatter_(1, label_map.data.long().cuda(), 1.0)\n",
    "\n",
    "    masked_label= torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n",
    "    masked_label=masked_label.scatter_(1,(label_map*(1-clothes_mask)).data.long().cuda(), 1.0)\n",
    "\n",
    "    c_label=torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n",
    "    c_label=c_label.scatter_(1,all_clothes_label.data.long().cuda(),1.0)\n",
    "\n",
    "    input_label = Variable(input_label)\n",
    "\n",
    "    return input_label,masked_label,c_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e24ccbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_discrete_label(inputs, label_nc, onehot=True):\n",
    "    pred_batch = []\n",
    "    size = inputs.size()\n",
    "    for input in inputs:\n",
    "        input = input.view(1, label_nc, size[2], size[3])\n",
    "        pred = np.squeeze(input.data.max(1)[1].cpu().numpy(), axis=0)\n",
    "        pred_batch.append(pred)\n",
    "    pred_batch = np.array(pred_batch)\n",
    "    pred_batch = torch.from_numpy(pred_batch)\n",
    "    label_map = []\n",
    "    for p in pred_batch:\n",
    "        p = p.view(1, 256, 192)\n",
    "        label_map.append(p)\n",
    "    label_map = torch.stack(label_map, 0)\n",
    "    if not onehot:\n",
    "        return label_map.float().cuda()\n",
    "    size = label_map.size()\n",
    "    oneHot_size = (size[0], label_nc, size[2], size[3])\n",
    "    input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n",
    "    input_label = input_label.scatter_(1, label_map.data.long().cuda(), 1.0)\n",
    "    return input_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fad492b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def morpho(mask, iter, bigger=True):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    new = []\n",
    "    for i in range(len(mask)):\n",
    "        tem = mask[i].cpu().detach().numpy().squeeze().reshape(256, 192, 1)*255\n",
    "        tem = tem.astype(np.uint8)\n",
    "        if bigger:\n",
    "            tem = cv2.dilate(tem, kernel, iterations=iter)\n",
    "        else:\n",
    "            tem = cv2.erode(tem, kernel, iterations=iter)\n",
    "        tem = tem.astype(np.float64)\n",
    "        tem = tem.reshape(1, 256, 192)\n",
    "        new.append(tem.astype(np.float64)/255.0)\n",
    "    new = np.stack(new)\n",
    "    new = torch.FloatTensor(new).cuda()\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889dd7e1",
   "metadata": {},
   "source": [
    "![flow](stn_diagram.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12b238",
   "metadata": {},
   "source": [
    "![](affine_vs_tps.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "527a6a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(5, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(54900, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(5, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 60 * 44, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        # xs is the source control point\n",
    "        xs = xs.view(-1, 10 * 60 * 44)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transform the input\n",
    "        x = self.stn(x)\n",
    "        print(x.size())\n",
    "        # x has already gone passed affine transformation \n",
    "        # Perform the usual forward pass\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 54900)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = Net().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033cf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f48e78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for data in dataloader:\n",
    "        in_label = Variable(data['label'].cuda())\n",
    "        in_edge = Variable(data['edge'].cuda())\n",
    "        in_color = Variable(data['color'].cuda())\n",
    "        pre_clothes_mask = torch.FloatTensor((in_edge.detach().cpu().numpy() > 0.5).astype(np.float)).cuda()\n",
    "        clothes = in_color*pre_clothes_mask\n",
    "        fake_cl = data['msc'][0]\n",
    "        fake_cl_dis = torch.FloatTensor((fake_cl.detach().cpu().numpy() > 0.5).astype(np.float)).cuda()\n",
    "        fake_cl_dis = morpho(fake_cl_dis, 1, True)\n",
    "        \n",
    "        optimizer.zero_grad()     \n",
    "        data = torch.cat([pre_clothes_mask, fake_cl_dis, clothes], 1).cuda()\n",
    "        output = model(data)\n",
    "        \n",
    "        output = F.grid_sample(in_label, output)\n",
    "        print(output.size())\n",
    "\n",
    "        '''loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))'''\n",
    "        \n",
    "        break\n",
    "\n",
    "def test():\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "              .format(test_loss, correct, len(test_loader.dataset),\n",
    "                      100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfb6d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a7e6111d68e9>:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pre_clothes_mask = torch.FloatTensor((in_edge.detach().cpu().numpy() > 0.5).astype(np.float)).cuda()\n",
      "<ipython-input-11-a7e6111d68e9>:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fake_cl_dis = torch.FloatTensor((fake_cl.detach().cpu().numpy() > 0.5).astype(np.float)).cuda()\n",
      "C:\\Users\\Tasin\\anaconda3\\envs\\improve\\lib\\site-packages\\torch\\nn\\functional.py:3890: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tasin\\anaconda3\\envs\\improve\\lib\\site-packages\\torch\\nn\\functional.py:3828: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 256, 192])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grid_sampler(): expected 4D or 5D input and grid with same number of dimensions, but got input with sizes [1, 1, 256, 192] and grid with sizes [1, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b888f18a64ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-a7e6111d68e9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\improve\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mgrid_sample\u001b[1;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[0;32m   3834\u001b[0m         \u001b[0malign_corners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3835\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3836\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_sampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode_enum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_mode_enum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grid_sampler(): expected 4D or 5D input and grid with same number of dimensions, but got input with sizes [1, 1, 256, 192] and grid with sizes [1, 10]"
     ]
    }
   ],
   "source": [
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a336210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def convert_image_np(inp):\n",
    "    \"\"\"Convert a Tensor to numpy image.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    return inp\n",
    "\n",
    "def visualize_stn():\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of training data\n",
    "        data = torch.cat([pre_clothes_mask, fake_cl_dis, clothes], 1).cuda()\n",
    "\n",
    "        input_tensor = data.cpu()\n",
    "        transformed_input_tensor = model.stn(data).cpu()\n",
    "\n",
    "        in_grid = convert_image_np(\n",
    "            torchvision.utils.make_grid(input_tensor))\n",
    "\n",
    "        out_grid = convert_image_np(\n",
    "            torchvision.utils.make_grid(transformed_input_tensor))\n",
    "\n",
    "        # Plot the results side-by-side\n",
    "        f, axarr = plt.subplots(1, 2)\n",
    "        axarr[0].imshow(in_grid)\n",
    "        axarr[0].set_title('Dataset Images')\n",
    "\n",
    "        axarr[1].imshow(out_grid)\n",
    "        axarr[1].set_title('Transformed Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_stn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_input_tensor = model.stn(data).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 10))\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "plt.imshow(pre_clothes_mask[0].permute(1,2,0).detach().cpu().numpy())\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "plt.imshow(fake_cl_dis[0].permute(1,2,0).detach().cpu().numpy())\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "plt.imshow(clothes[0].permute(1,2,0).detach().cpu().numpy()+1)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "print(pre_clothes_mask.size())\n",
    "print(fake_cl_dis.size())\n",
    "print(clothes.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = torch.cat([pre_clothes_mask, fake_cl_dis, clothes], 1).cuda()\n",
    "plt.imshow(in_color[0].permute(1,2,0).detach().cpu().numpy()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56abb8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = stn(in_color, reference, pre_clothes_mask, in_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf50c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "improve",
   "language": "python",
   "name": "improve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
